# NLP_Paper_Pool
<!-- TABLE OF CONTENTS -->
## Table of Contents
* [Machine Translation](#machine-translation)
* [Machine Translation (Non-Autoregressive)](#machine-translation-non-autoregressive)
* [Machine Translation (Low-Resource)](#machine-translation-low-resource)
* [Knowledge Distillation](#knowledge-distillation)
* [Attention](#attention)
* [Transformers](#transformers)
* [Training Tips for Transformers](#training-tips-for-transformers)
  * [Positional Encoding](#positional-encoding)
  * [Long Text](#long-text)
  * [Miscellaneous](#miscellaneous)
* [Explaination](#explaination)
* [Rich Answer Type](#rich-answer-type)
* [Optimizer](#optimizer)

<!-- Machine Translation -->
## Machine Translation
- October 2020: [Multi-task Learning for Multilingual Neural Machine Translation](https://arxiv.org/abs/2010.02523)
- September 2020: [Energy-Based Reranking: Improving Neural Machine Translation Using Energy-Based Models](https://arxiv.org/abs/2009.13267)
- September 2020: [Softmax Tempering for Training Neural Machine Translation Models](https://arxiv.org/abs/2009.09372)
- September 2020: [CSP: Code-Switching Pre-training for Neural Machine Translation](https://arxiv.org/abs/2009.08088)
- June 2020: [Deep Encoder, Shallow Decoder: Reevaluating the Speed-Quality Tradeoff in Machine Translation](https://arxiv.org/abs/2006.10369)

<!-- Machine Translation (Non-Autoregressive)-->
## Machine Translation (Non-Autoregressive)
- April 2020: [Non-Autoregressive Machine Translation with Latent Alignments](https://arxiv.org/abs/2004.07437)

<!-- Machine Translation (Low-Resource)-->
## Machine Translation (Low-Resource)
- October 2020: [Improving Target-side Lexical Transfer in Multilingual Neural Machine Translation](https://arxiv.org/abs/2010.01667)
- September 2020: [Harnessing Multilinguality in Unsupervised Machine Translation for Rare Languages](https://arxiv.org/abs/2009.11201)

<!-- Knowledge Distillation -->
## Knowledge Distillation
- September 2020: [Contrastive Distillation on Intermediate Representations for Language Model Compression](https://arxiv.org/abs/2009.14167v1)
- September 2020: [Weight Distillation: Transferring the Knowledge in Neural Network Parameters](https://arxiv.org/abs/2009.09152)
- June 2020: [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316)
- February 2020: [BERT-of-Theseus: Compressing BERT by Progressive Module Replacing](https://arxiv.org/abs/2002.02925)

<!-- Attention -->
## Attention
- September 2020: [Sparsifying Transformer Models with Differentiable Representation Pooling](https://arxiv.org/abs/2009.05169)
- September 2020: [Repulsive Attention: Rethinking Multi-head Attention as Bayesian Inference](https://arxiv.org/abs/2009.09364)
- June 2020: [Limits to Depth Efficiencies of Self-Attention](https://arxiv.org/abs/2006.12467)
- May 2020: [Hard-Coded Gaussian Attention for Neural Machine Translation](https://arxiv.org/abs/2005.00742)
- November 2019: [Location Attention for Extrapolation to Longer Sequences](https://arxiv.org/abs/1911.03872)

<!-- Transforemrs -->
## Transformers
- April 2020: [Fast and Accurate Deep Bidirectional Language Representations for Unsupervised Learning](https://arxiv.org/abs/2004.08097)
- May 2019: [Unified Language Model Pre-training for Natural Language Understanding and Generation](https://arxiv.org/abs/1905.03197)

<!-- Traning Tips for Transformers -->
## Training Tips for Transformers
### Positional Encoding
- March 2020: [Learning to Encode Position for Transformer with Continuous Dynamical Model](https://arxiv.org/abs/2003.09229)
### Long Text
- June 2020: [Progressive Generation of Long Text](https://arxiv.org/abs/2006.15720)
### Miscellaneous
- July 2020: [Data Movement Is All You Need: A Case Study on Optimizing Transformers](https://arxiv.org/abs/2007.00072)

<!-- Explaination -->
## Explaination
- April 2020: [Attention Module is Not Only a Weight: Analyzing Transformers with Vector Norms](https://arxiv.org/abs/2004.10102)
- September 2019: [Learning to Deceive with Attention-Based Explanations](https://arxiv.org/abs/1909.07913)

<!-- Rich Answer Type -->
## Rich Answer Type
- September 2020:[No Answer is Better Than Wrong Answer: A Reflection Model for Document Level Machine Reading Comprehension](https://arxiv.org/abs/2009.12056)

<!-- Optimizer -->
## Optimizer
- September 2020:[Apollo: An Adaptive Parameter-wise Diagonal Quasi-Newton Method for Nonconvex Stochastic Optimization](https://arxiv.org/abs/2009.13586)
